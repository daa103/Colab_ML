{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daa103/DAA_ESAA/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "daebf58e-ea4b-44fa-fc98-2cdc61cf014b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(text=text)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "GowligokZeEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7480c4b7-91ab-42d8-de67-1a834d65b99a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['Hi, My name is Amartya Nambiar.', 'I am a Computer Science Engineer.', 'My favourite color is black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "words = word_tokenize(text)\n",
        "print(len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9be73d5-1ae3-4b7a-d2ea-7b8277c190f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.', 'I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'is', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0f013d-0558-447d-b4e9-57d138bf55e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "nltk.corpus.stopwords.words('english')[:15]"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93d8f38-ab83-41e7-c58a-472baa261bc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "all_tokens=[]\n",
        "# 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반목문\n",
        "for word in words:\n",
        "  # 소문자로 모두 변환합니다.\n",
        "  word=word.lower()\n",
        "  # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "  if word not in stopwords:\n",
        "    all_tokens.append(word)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a828c954-2fb8-4259-81dc-ec08eb8c6520"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', ',', 'name', 'amartya', 'nambiar', '.', 'computer', 'science', 'engineer', '.', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "new_tokens=[]\n",
        "for word in all_tokens:\n",
        "  if word not in [',', '.']:\n",
        "    new_tokens.append(word)"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "# example1= ['helps', 'helping', 'helped']\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem('helps'), ps.stem('helping'), ps.stem('helped'))\n",
        "print(ps.stem('working'), ps.stem('works'), ps.stem('worked'))\n",
        "print(ps.stem('amusing'), ps.stem('amused'), ps.stem('amuses'))\n",
        "print(ps.stem('happier'), ps.stem('happiest'))"
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09acee83-6475-4915-d689-21fff84a982c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "help help help\n",
            "work work work\n",
            "amus amus amus\n",
            "happier happiest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('happiness') # but it isn't always the best choice"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecf7e7e3-f07e-452f-80da-fa09f73703df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective."
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RiQyC8ot8UV",
        "outputId": "6e738055-cb9d-409c-edd8-5f49797caf99"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\")"
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QckotM9vsc9J",
        "outputId": "d7be2a50-7209-4ee4-ac70-369119e17e90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply pos_tag(), print result\n",
        "words = word_tokenize(text)\n",
        "\n",
        "all_tokens=[]\n",
        "for word in words:\n",
        "  word=word.lower()\n",
        "  if word not in stopwords:\n",
        "    all_tokens.append(word)\n",
        "\n",
        "new_tokens=[]\n",
        "for word in all_tokens:\n",
        "  if word not in [',', '.']:\n",
        "    new_tokens.append(word)\n",
        "\n",
        "print(pos_tag(new_tokens))"
      ],
      "metadata": {
        "id": "vcWgmScAedLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c65247-603d-4574-c0a7-5f7110a3dbe0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('new', 'JJ'), ('entry', 'NN'), ('``', '``'), ('revisionist', 'JJ'), ('history', 'NN'), ('``', '``'), ('genre', 'JJ'), ('filmmaking', 'NN'), ('dick', 'JJ'), ('suggests', 'VBZ'), ('two', 'CD'), ('not-too-bright', 'JJ'), ('teenage', 'NN'), ('girls', 'NNS'), ('cause', 'VBP'), ('uncovering', 'VBG'), ('nation', 'NN'), (\"'s\", 'POS'), ('biggest', 'JJS'), ('presidential', 'JJ'), ('scandal', 'NN'), ('kirsten', 'VBN'), ('dunst', 'JJ'), ('michelle', 'NN'), ('williams', 'NNS'), ('star', 'VBP'), ('betsy', 'JJ'), ('arlene', 'NN'), ('trying', 'VBG'), ('deliver', 'VB'), ('fan', 'JJ'), ('letter', 'NN'), ('arlene', 'NN'), (\"'s\", 'POS'), ('watergate', 'JJ'), ('hotel', 'NN'), ('room', 'NN'), ('accidentally', 'RB'), ('stumble', 'JJ'), ('across', 'IN'), ('g', 'JJ'), ('gordon', 'NN'), ('liddy', 'NN'), ('(', '('), ('played', 'VBN'), ('dead-on', 'NN'), ('harry', 'NN'), ('shearer', 'NN'), (')', ')'), ('infamous', 'JJ'), ('break-in', 'JJ'), ('recognize', 'NN'), ('liddy', 'NN'), ('later', 'RB'), ('white', 'JJ'), ('house', 'NN'), ('field', 'NN'), ('trip', 'NN'), ('ushered', 'JJ'), ('conference', 'NN'), ('room', 'NN'), ('questioned', 'VBD'), ('know', 'NNS'), ('leave', 'VBP'), ('official', 'JJ'), ('presidential', 'JJ'), ('dog', 'NN'), ('walkers', 'NNS'), ('girls', 'VBP'), ('manage', 'NN'), ('unwittingly', 'RB'), ('uncover', 'RB'), ('every', 'DT'), ('bit', 'NN'), ('watergate', 'JJ'), ('scandal', 'NN'), ('performing', 'VBG'), ('duties', 'NNS'), ('clue', 'VBP'), ('getting', 'VBG'), ('involved', 'VBN'), ('discover', 'RB'), ('nixon', 'RB'), ('(', '('), ('another', 'DT'), ('dead-on', 'JJ'), ('performance', 'NN'), ('dan', 'NN'), ('hedaya', 'NN'), ('actually', 'RB'), ('favors', 'VBZ'), ('nixon', 'JJ'), ('slightly', 'RB'), ('unlike', 'IN'), ('anthony', 'NN'), ('hopkins', 'NNS'), (')', ')'), ('abusive', 'VBP'), ('checkers', 'NNS'), ('presidential', 'JJ'), ('dog', 'NN'), ('thanks', 'NNS'), ('conversations', 'NNS'), ('always', 'RB'), ('recorded', 'VBD'), ('quit', 'NN'), ('become', 'NN'), ('disillusioned', 'VBD'), ('prank', 'JJ'), ('phone', 'NN'), ('call', 'NN'), ('girls', 'NNS'), ('make', 'VBP'), ('woodward', 'JJ'), ('bernstein', 'NN'), ('events', 'NNS'), ('set', 'VBN'), ('motion', 'NN'), ('eventually', 'RB'), ('lead', 'JJ'), ('president', 'NN'), (\"'s\", 'POS'), ('resignation', 'NN'), ('film', 'NN'), ('starts', 'VBZ'), ('promisingly', 'RB'), ('aged', 'VBN'), ('woodward', 'NN'), ('bernstein', 'NN'), ('arguing', 'VBG'), ('obvious', 'JJ'), ('larry', 'JJ'), ('king-type', 'JJ'), ('talk', 'NN'), ('show', 'NN'), ('(', '('), ('featuring', 'VBG'), ('cameo', 'NN'), ('french', 'JJ'), ('stewart', 'NN'), (')', ')'), ('revealing', 'NN'), ('identity', 'NN'), ('``', '``'), ('deep', 'JJ'), ('throat', 'NN'), ('``', '``'), ('subjected', 'VBN'), ('bodily', 'RB'), ('function', 'NN'), ('humor', 'NN'), ('every', 'DT'), ('bad', 'JJ'), ('``', '``'), ('dick', 'JJ'), ('``', '``'), ('joke', 'VBD'), ('one', 'CD'), ('derive', 'NN'), ('type', 'NN'), ('supposed', 'VBD'), ('comedy', 'NN'), ('one', 'CD'), ('point', 'NN'), ('girls', 'NNS'), ('scream', 'VBP'), ('high', 'JJ'), ('school', 'NN'), ('band', 'NN'), ('playing', 'VBG'), ('steps', 'NNS'), ('lincoln', 'RB'), ('memorial', 'JJ'), ('band', 'NN'), ('manages', 'NNS'), ('stop', 'VB'), ('right', 'NN'), ('dunst', 'NN'), ('screams', 'VBZ'), ('``', '``'), ('stop', 'VB'), ('letting', 'VBG'), ('dick', 'JJ'), ('run', 'NN'), ('life', 'NN'), ('!', '.'), ('``', '``'), ('much', 'JJ'), ('horror', 'NN'), ('everyone', 'NN'), ('standing', 'VBG'), ('within', 'IN'), ('earshot', 'JJ'), ('several', 'JJ'), ('variations', 'NNS'), ('wordplay', 'VBP'), ('surface', 'NN'), ('throughout', 'IN'), ('film', 'NN'), ('movie', 'NN'), ('smarter', 'NN'), ('would', 'MD'), ('less', 'RBR'), ('likely', 'JJ'), ('fault', 'NN'), (\"'s\", 'POS'), ('juvenile', 'NN'), ('bathroom', 'NN'), ('humor', 'NN'), (\"'s\", 'POS'), ('film', 'NN'), ('apparently', 'RB'), ('made', 'VBD'), ('relatively', 'RB'), ('younger', 'JJ'), ('people', 'NNS'), ('every', 'DT'), ('major', 'JJ'), ('player', 'NN'), ('watergate', 'NN'), ('scandal', 'NN'), ('introduced', 'VBD'), ('shoved', 'JJ'), ('audience', 'NN'), (\"'s\", 'POS'), ('throat', 'NN'), ('least', 'VBD'), ('subtle', 'JJ'), ('way', 'NN'), ('possible', 'JJ'), (\"n't\", 'RB'), ('recall', 'VB'), ('oliver', 'RP'), ('stone', 'NN'), (\"'s\", 'POS'), ('nixon', 'JJ'), ('pander', 'NN'), (\"'s\", 'POS'), ('audience', 'NN'), ('course', 'NN'), ('film', 'NN'), (\"n't\", 'RB'), ('comedy', 'NN'), ('aimed', 'VBN'), ('squarely', 'RB'), ('13-20', 'JJ'), ('year-old', 'JJ'), ('film', 'NN'), ('going', 'VBG'), ('audience', 'NN'), ('redeeming', 'VBG'), ('thing', 'NN'), ('movie', 'NN'), (\"'s\", 'POS'), ('remarkable', 'JJ'), ('supporting', 'VBG'), ('cast', 'NN'), ('wanted', 'VBD'), ('see', 'NN'), ('ferrell', 'NN'), ('mcculloch', 'NN'), (\"'s\", 'POS'), ('woodward', 'JJ'), ('bernstein', 'NN'), ('two', 'CD'), ('characters', 'NNS'), ('sole', 'JJ'), ('basis', 'NN'), ('rating', 'NN'), ('wish', 'JJ'), ('given', 'VBN'), ('screen', 'JJ'), ('time', 'NN'), ('unfortunately', 'RB'), ('relegated', 'VBD'), ('final', 'JJ'), ('half-hour', 'NN'), ('constant', 'JJ'), ('bickering', 'NN'), ('fighting', 'VBG'), ('trying', 'VBG'), ('get', 'VB'), ('story', 'NN'), ('major', 'JJ'), ('highlight', 'NN'), ('especially', 'RB'), ('mcculloch', 'VBP'), (\"'s\", 'POS'), ('constant', 'JJ'), ('thwarting', 'NN'), ('ferrell', 'NN'), (\"'s\", 'POS'), ('attempts', 'NNS'), ('gather', 'CC'), ('information', 'NN'), ('girls', 'NNS'), ('(', '('), ('course', 'NN'), ('narrative', 'RB'), ('revealed', 'VBD'), ('deep', 'JJ'), ('throat', 'NN'), ('named', 'VBN'), ('thanks', 'NNS'), ('ill', 'VBP'), ('planned', 'VBN'), ('trip', 'NN'), ('porno', 'NN'), ('theater', 'NN'), ('betsy', 'NN'), (\"'s\", 'POS'), ('brother', 'NN'), (')', ')'), ('members', 'NNS'), ('cast', 'VBD'), ('excellent', 'JJ'), ('portrayals', 'NNS'), ('particular', 'JJ'), ('characters', 'NNS'), ('given', 'VBN'), ('nothing', 'NN'), ('work', 'NN'), (\"'d\", 'MD'), ('like', 'VB'), ('see', 'VB'), ('cast', 'JJ'), ('portray', 'NN'), ('characters', 'NNS'), ('script', 'VBP'), ('suited', 'VBN'), ('towards', 'NNS'), ('comedic', 'JJ'), ('abilities', 'NNS'), ('two', 'CD'), ('leads', 'NNS'), ('dunst', 'JJ'), ('williams', 'NNS'), ('definitely', 'RB'), ('better', 'RBR'), ('come', 'VB'), ('could', 'MD'), ('best', 'VB'), ('described', 'JJ'), ('romy', 'JJ'), ('michele', 'NN'), (':', ':'), ('early', 'JJ'), ('years', 'NNS'), ('particular', 'JJ'), ('film', 'NN'), ('highly', 'RB'), ('dubious', 'JJ'), ('distinction', 'NN'), ('best', 'JJS'), ('stay', 'NN'), ('first', 'RB'), ('half', 'JJ'), ('end', 'NN'), ('credits', 'NNS'), ('though', 'IN'), ('see', 'VBP'), ('interesting', 'JJ'), ('scene', 'NN'), ('involving', 'VBG'), ('dunst', 'JJ'), ('williams', 'NNS'), ('suggestively', 'RB'), ('sucking', 'VBG'), ('lollipops', 'NNS'), ('emblazoned', 'VBD'), ('title', 'JJ'), ('movie', 'NN'), ('excellent', 'JJ'), ('idea', 'NN'), ('marred', 'VBD'), ('poor', 'JJ'), ('execution', 'NN'), ('dick', 'NN'), ('could', 'MD'), ('great', 'JJ'), ('movie', 'NN'), ('less', 'RBR'), ('juvenile', 'JJ'), ('humor', 'NN'), ('smarter', 'RBR'), ('comedy', 'NN'), ('displayed', 'VBN'), ('woodward', 'IN'), ('bernstein', 'NN'), ('scenes', 'NNS'), ('could', 'MD'), ('made', 'VB'), ('film', 'NN'), ('wonderful', 'JJ'), ('satire', 'NN'), ('nixon', 'NN'), ('presidency', 'NN'), ('seen', 'VBN'), ('eyes', 'NNS'), ('two', 'CD'), ('naive', 'JJ'), ('fifteen', 'JJ'), ('year', 'NN'), ('olds', 'NNS'), ('stands', 'VBZ'), ('though', 'IN'), ('dick', 'JJ'), ('offers', 'NNS'), ('nothing', 'NN'), ('filmmaker', 'NN'), ('kevin', 'NN'), ('smith', 'NN'), ('accurately', 'RB'), ('defines', 'VBZ'), ('``', '``'), ('dick', 'JJ'), ('poopie', 'NN'), ('``', '``'), ('jokes', 'NNS'), ('make', 'VBP'), ('funny', 'JJ'), ('movie', 'NN'), ('[', 'NNP'), ('pg-13', 'NN'), (']', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb19d7fe-845c-472d-9639-3987bbea1da2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('believes','v'))\n",
        "print(lemma.lemmatize('happiness','a'))"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778b4502-7d2f-46cb-ee53-8caefac1ee91"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "believe\n",
            "happiness\n"
          ]
        }
      ]
    }
  ]
}